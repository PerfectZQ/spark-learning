17/04/10 11:06:03 INFO zookeeper.ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
17/04/10 11:06:03 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
17/04/10 11:06:03 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
17/04/10 11:06:03 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux
17/04/10 11:06:03 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd64
17/04/10 11:06:03 INFO zookeeper.ZooKeeper: Client environment:os.version=2.6.32-642.el6.x86_64
17/04/10 11:06:03 INFO zookeeper.ZooKeeper: Client environment:user.name=neu
17/04/10 11:06:03 INFO zookeeper.ZooKeeper: Client environment:user.home=/home/neu
17/04/10 11:06:03 INFO zookeeper.ZooKeeper: Client environment:user.dir=/opt/neu/spark-2.0.2-bin-without-hadoop/conf
17/04/10 11:06:03 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=s12181:2181,s12191:2181,s12192:2181 sessionTimeout=60000 watcher=hconnection-0x431db6880x0, quorum=s12181:2181,s12191:2181,s12192:2181, baseZNode=/hbase
17/04/10 11:06:03 INFO zookeeper.ClientCnxn: Opening socket connection to server s12181/10.4.121.81:2181. Will not attempt to authenticate using SASL (unknown error)
17/04/10 11:06:03 INFO zookeeper.ClientCnxn: Socket connection established, initiating session, client: /10.4.120.83:60890, server: s12181/10.4.121.81:2181
17/04/10 11:06:03 INFO zookeeper.ClientCnxn: Session establishment complete on server s12181/10.4.121.81:2181, sessionid = 0x15b5586b96d00a9, negotiated timeout = 60000
17/04/10 11:06:03 INFO util.RegionSizeCalculator: Calculating region sizes for table "ci_user_KUWL3pPy".
17/04/10 11:06:04 INFO client.ConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
17/04/10 11:06:04 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x15b5586b96d00a9
17/04/10 11:06:04 INFO zookeeper.ZooKeeper: Session: 0x15b5586b96d00a9 closed
17/04/10 11:06:04 INFO zookeeper.ClientCnxn: EventThread shut down
17/04/10 11:06:04 INFO zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x244ff5d9 connecting to ZooKeeper ensemble=s12181:2181,s12191:2181,s12192:2181
17/04/10 11:06:04 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=s12181:2181,s12191:2181,s12192:2181 sessionTimeout=60000 watcher=hconnection-0x244ff5d90x0, quorum=s12181:2181,s12191:2181,s12192:2181, baseZNode=/hbase
17/04/10 11:06:04 INFO zookeeper.ClientCnxn: Opening socket connection to server s12192/10.4.121.92:2181. Will not attempt to authenticate using SASL (unknown error)
17/04/10 11:06:04 INFO zookeeper.ClientCnxn: Socket connection established, initiating session, client: /10.4.120.83:54328, server: s12192/10.4.121.92:2181
17/04/10 11:06:04 INFO zookeeper.ClientCnxn: Session establishment complete on server s12192/10.4.121.92:2181, sessionid = 0x35b5586b83d00b8, negotiated timeout = 60000
17/04/10 11:06:04 INFO util.RegionSizeCalculator: Calculating region sizes for table "ci_result_KUWL3pPy".
17/04/10 11:06:04 INFO client.ConnectionManager$HConnectionImplementation: Closing master protocol: MasterService
17/04/10 11:06:04 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x35b5586b83d00b8
17/04/10 11:06:04 INFO zookeeper.ZooKeeper: Session: 0x35b5586b83d00b8 closed
17/04/10 11:06:04 INFO zookeeper.ClientCnxn: EventThread shut down
17/04/10 11:06:04 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/04/10 11:06:04 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/04/10 11:06:04 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/04/10 11:06:04 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/04/10 11:06:04 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/04/10 11:06:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
17/04/10 11:06:04 WARN output.FileOutputCommitter: Output Path is null in setupJob()
17/04/10 11:06:04 INFO spark.SparkContext: Starting job: saveAsHadoopDataset at Client.scala:117
17/04/10 11:06:04 INFO scheduler.DAGScheduler: Registering RDD 4 (union at Client.scala:85)
17/04/10 11:06:04 INFO scheduler.DAGScheduler: Got job 0 (saveAsHadoopDataset at Client.scala:117) with 7 output partitions
17/04/10 11:06:04 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (saveAsHadoopDataset at Client.scala:117)
17/04/10 11:06:04 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/04/10 11:06:04 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/04/10 11:06:04 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (UnionRDD[4] at union at Client.scala:85), which has no missing parents
17/04/10 11:06:04 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.6 KB, free 365.8 MB)
17/04/10 11:06:04 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.1 KB, free 365.8 MB)
17/04/10 11:06:04 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.4.120.83:48045 (size: 3.1 KB, free: 366.2 MB)
17/04/10 11:06:04 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1012
17/04/10 11:06:04 INFO scheduler.DAGScheduler: Submitting 7 missing tasks from ShuffleMapStage 0 (UnionRDD[4] at union at Client.scala:85)
17/04/10 11:06:04 INFO cluster.YarnScheduler: Adding task set 0.0 with 7 tasks
17/04/10 11:06:09 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.4.121.92:50687) with ID 1
17/04/10 11:06:09 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, s12192, partition 0, NODE_LOCAL, 5621 bytes)
17/04/10 11:06:09 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 1, s12192, partition 6, NODE_LOCAL, 5587 bytes)
17/04/10 11:06:09 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 0 on executor id: 1 hostname: s12192.
17/04/10 11:06:09 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 1 on executor id: 1 hostname: s12192.
17/04/10 11:06:09 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.4.121.91:41647) with ID 2
17/04/10 11:06:09 INFO storage.BlockManagerMasterEndpoint: Registering block manager s12192:52902 with 2004.6 MB RAM, BlockManagerId(1, s12192, 52902)
17/04/10 11:06:09 INFO storage.BlockManagerMasterEndpoint: Registering block manager s12191:58443 with 2004.6 MB RAM, BlockManagerId(2, s12191, 58443)
17/04/10 11:06:10 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on s12192:52902 (size: 3.1 KB, free: 2004.6 MB)
17/04/10 11:06:11 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on s12192:52902 (size: 30.1 KB, free: 2004.6 MB)
17/04/10 11:06:11 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on s12192:52902 (size: 30.0 KB, free: 2004.5 MB)
17/04/10 11:06:13 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 2, s12192, partition 1, RACK_LOCAL, 5653 bytes)
17/04/10 11:06:13 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 3, s12191, partition 2, RACK_LOCAL, 5653 bytes)
17/04/10 11:06:13 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 4, s12192, partition 3, RACK_LOCAL, 5653 bytes)
17/04/10 11:06:13 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 5, s12191, partition 4, RACK_LOCAL, 5653 bytes)
17/04/10 11:06:13 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 6, s12192, partition 5, RACK_LOCAL, 5621 bytes)
17/04/10 11:06:13 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 2 on executor id: 1 hostname: s12192.
17/04/10 11:06:13 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 4 on executor id: 1 hostname: s12192.
17/04/10 11:06:13 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 6 on executor id: 1 hostname: s12192.
17/04/10 11:06:13 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 3 on executor id: 2 hostname: s12191.
17/04/10 11:06:13 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 5 on executor id: 2 hostname: s12191.
17/04/10 11:06:13 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on s12191:58443 (size: 3.1 KB, free: 2004.6 MB)
17/04/10 11:06:14 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on s12191:58443 (size: 30.0 KB, free: 2004.6 MB)
17/04/10 11:06:14 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 1) in 5143 ms on s12192 (1/7)
17/04/10 11:09:28 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 199260 ms on s12192 (2/7)
17/04/10 11:09:39 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 3) in 206449 ms on s12191 (3/7)
17/04/10 11:10:02 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 2) in 229786 ms on s12192 (4/7)
17/04/10 11:12:36 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 4) in 383484 ms on s12192 (5/7)
17/04/10 11:16:16 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 6) in 603229 ms on s12192 (6/7)
17/04/10 11:16:48 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 5) in 635315 ms on s12191 (7/7)
17/04/10 11:16:48 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (union at Client.scala:85) finished in 643.440 s
17/04/10 11:16:48 INFO scheduler.DAGScheduler: looking for newly runnable stages
17/04/10 11:16:48 INFO scheduler.DAGScheduler: running: Set()
17/04/10 11:16:48 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)
17/04/10 11:16:48 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/04/10 11:16:48 INFO scheduler.DAGScheduler: failed: Set()
17/04/10 11:16:48 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at flatMap at Client.scala:89), which has no missing parents
17/04/10 11:16:48 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 91.6 KB, free 365.7 MB)
17/04/10 11:16:49 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.9 KB, free 365.6 MB)
17/04/10 11:16:49 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.4.120.83:48045 (size: 34.9 KB, free: 366.2 MB)
17/04/10 11:16:49 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1012
17/04/10 11:16:49 INFO scheduler.DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at flatMap at Client.scala:89)
17/04/10 11:16:49 INFO cluster.YarnScheduler: Adding task set 1.0 with 7 tasks
17/04/10 11:16:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 7, s12192, partition 0, NODE_LOCAL, 5241 bytes)
17/04/10 11:16:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 8, s12191, partition 1, NODE_LOCAL, 5241 bytes)
17/04/10 11:16:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 9, s12192, partition 2, NODE_LOCAL, 5241 bytes)
17/04/10 11:16:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 10, s12191, partition 3, NODE_LOCAL, 5241 bytes)
17/04/10 11:16:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 1.0 (TID 11, s12192, partition 4, NODE_LOCAL, 5241 bytes)
17/04/10 11:16:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 1.0 (TID 12, s12191, partition 5, NODE_LOCAL, 5241 bytes)
17/04/10 11:16:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 1.0 (TID 13, s12192, partition 6, NODE_LOCAL, 5241 bytes)
17/04/10 11:16:49 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 7 on executor id: 1 hostname: s12192.
17/04/10 11:16:49 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 9 on executor id: 1 hostname: s12192.
17/04/10 11:16:49 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 11 on executor id: 1 hostname: s12192.
17/04/10 11:16:49 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 13 on executor id: 1 hostname: s12192.
17/04/10 11:16:49 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 8 on executor id: 2 hostname: s12191.
17/04/10 11:16:49 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 10 on executor id: 2 hostname: s12191.
17/04/10 11:16:49 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 12 on executor id: 2 hostname: s12191.
17/04/10 11:16:49 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on s12191:58443 (size: 34.9 KB, free: 2004.5 MB)
17/04/10 11:16:49 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on s12192:52902 (size: 34.9 KB, free: 2004.5 MB)
17/04/10 11:16:49 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.4.121.92:50687
17/04/10 11:16:49 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.4.121.91:41647
17/04/10 11:16:49 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 183 bytes
17/04/10 11:20:35 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 7, s12192): com.mongodb.MongoSocketReadException: Prematurely reached end of stream
	at com.mongodb.connection.SocketStream.read(SocketStream.java:88)
	at com.mongodb.connection.InternalStreamConnection.receiveResponseBuffers(InternalStreamConnection.java:491)
	at com.mongodb.connection.InternalStreamConnection.receiveMessage(InternalStreamConnection.java:221)
	at com.mongodb.connection.CommandHelper.receiveReply(CommandHelper.java:134)
	at com.mongodb.connection.CommandHelper.receiveCommandResult(CommandHelper.java:121)
	at com.mongodb.connection.CommandHelper.executeCommand(CommandHelper.java:32)
	at com.mongodb.connection.InternalStreamConnectionInitializer.initializeConnectionDescription(InternalStreamConnectionInitializer.java:83)
	at com.mongodb.connection.InternalStreamConnectionInitializer.initialize(InternalStreamConnectionInitializer.java:43)
	at com.mongodb.connection.InternalStreamConnection.open(InternalStreamConnection.java:115)
	at com.mongodb.connection.UsageTrackingInternalConnection.open(UsageTrackingInternalConnection.java:46)
	at com.mongodb.connection.DefaultConnectionPool$PooledConnection.open(DefaultConnectionPool.java:381)
	at com.mongodb.connection.DefaultConnectionPool.get(DefaultConnectionPool.java:96)
	at com.mongodb.connection.DefaultConnectionPool.get(DefaultConnectionPool.java:82)
	at com.mongodb.connection.DefaultServer.getConnection(DefaultServer.java:72)
	at com.mongodb.binding.ClusterBinding$ClusterBindingConnectionSource.getConnection(ClusterBinding.java:86)
	at com.mongodb.operation.OperationHelper.withConnectionSource(OperationHelper.java:237)
	at com.mongodb.operation.OperationHelper.withConnection(OperationHelper.java:212)
	at com.mongodb.operation.FindOperation.execute(FindOperation.java:482)
	at com.mongodb.operation.FindOperation.execute(FindOperation.java:79)
	at com.mongodb.Mongo.execute(Mongo.java:772)
	at com.mongodb.Mongo$2.execute(Mongo.java:759)
	at com.mongodb.DBCursor.initializeCursor(DBCursor.java:851)
	at com.mongodb.DBCursor.hasNext(DBCursor.java:152)
	at com.mongodb.casbah.MongoCursorBase$class.hasNext(MongoCursor.scala:73)
	at com.mongodb.casbah.MongoCursor.hasNext(MongoCursor.scala:487)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply$mcV$sp(PairRDDFunctions.scala:1203)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply(PairRDDFunctions.scala:1203)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$7.apply(PairRDDFunctions.scala:1203)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1211)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

17/04/10 11:20:35 INFO scheduler.TaskSetManager: Starting task 0.1 in stage 1.0 (TID 14, s12192, partition 0, NODE_LOCAL, 5241 bytes)
17/04/10 11:20:35 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 14 on executor id: 1 hostname: s12192.
17/04/10 11:20:35 INFO scheduler.TaskSetManager: Lost task 4.0 in stage 1.0 (TID 11) on executor s12192: com.mongodb.MongoSocketReadException (Prematurely reached end of stream) [duplicate 1]
17/04/10 11:20:35 INFO scheduler.TaskSetManager: Starting task 4.1 in stage 1.0 (TID 15, s12192, partition 4, NODE_LOCAL, 5241 bytes)
17/04/10 11:20:35 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 15 on executor id: 1 hostname: s12192.
17/04/10 11:20:35 INFO scheduler.TaskSetManager: Lost task 3.0 in stage 1.0 (TID 10) on executor s12191: com.mongodb.MongoSocketReadException (Prematurely reached end of stream) [duplicate 2]
17/04/10 11:20:35 INFO scheduler.TaskSetManager: Starting task 3.1 in stage 1.0 (TID 16, s12191, partition 3, NODE_LOCAL, 5241 bytes)
17/04/10 11:20:35 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 16 on executor id: 2 hostname: s12191.
17/04/10 11:20:35 INFO scheduler.TaskSetManager: Lost task 1.0 in stage 1.0 (TID 8) on executor s12191: com.mongodb.MongoSocketReadException (Prematurely reached end of stream) [duplicate 3]
17/04/10 11:20:35 INFO scheduler.TaskSetManager: Starting task 1.1 in stage 1.0 (TID 17, s12191, partition 1, NODE_LOCAL, 5241 bytes)
17/04/10 11:20:35 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 17 on executor id: 2 hostname: s12191.
17/04/10 11:20:36 INFO scheduler.TaskSetManager: Lost task 5.0 in stage 1.0 (TID 12) on executor s12191: com.mongodb.MongoSocketReadException (Prematurely reached end of stream) [duplicate 4]
17/04/10 11:20:36 INFO scheduler.TaskSetManager: Starting task 5.1 in stage 1.0 (TID 18, s12192, partition 5, NODE_LOCAL, 5241 bytes)
17/04/10 11:20:36 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 18 on executor id: 1 hostname: s12192.
17/04/10 11:20:38 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 1.
17/04/10 11:20:38 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 1)
17/04/10 11:20:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
17/04/10 11:20:38 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, s12192, 52902)
17/04/10 11:20:38 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
17/04/10 11:20:38 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 1)
17/04/10 11:20:38 INFO scheduler.ShuffleMapStage: ShuffleMapStage 0 is now unavailable on executor 1 (2/7, false)
17/04/10 11:20:38 ERROR cluster.YarnScheduler: Lost executor 1 on s12192: Container killed by YARN for exceeding memory limits. 6.0 GB of 6 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
17/04/10 11:20:38 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 1.0 (TID 14, s12192): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 6.0 GB of 6 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
17/04/10 11:20:38 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 1.0 (TID 13, s12192): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 6.0 GB of 6 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
17/04/10 11:20:38 WARN scheduler.TaskSetManager: Lost task 5.1 in stage 1.0 (TID 18, s12192): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 6.0 GB of 6 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
17/04/10 11:20:38 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 1.0 (TID 9, s12192): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 6.0 GB of 6 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
17/04/10 11:20:38 WARN scheduler.TaskSetManager: Lost task 4.1 in stage 1.0 (TID 15, s12192): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 6.0 GB of 6 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
17/04/10 11:20:38 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Container killed by YARN for exceeding memory limits. 6.0 GB of 6 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
17/04/10 11:20:38 INFO storage.BlockManagerMaster: Removal of executor 1 requested
17/04/10 11:20:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
17/04/10 11:20:38 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 1
17/04/10 11:20:39 INFO scheduler.TaskSetManager: Starting task 4.2 in stage 1.0 (TID 19, s12191, partition 4, NODE_LOCAL, 5241 bytes)
17/04/10 11:20:39 INFO scheduler.TaskSetManager: Starting task 2.1 in stage 1.0 (TID 20, s12191, partition 2, NODE_LOCAL, 5241 bytes)
17/04/10 11:20:39 INFO scheduler.TaskSetManager: Starting task 5.2 in stage 1.0 (TID 21, s12191, partition 5, NODE_LOCAL, 5241 bytes)
17/04/10 11:20:39 INFO scheduler.TaskSetManager: Starting task 6.1 in stage 1.0 (TID 22, s12191, partition 6, NODE_LOCAL, 5241 bytes)
17/04/10 11:20:39 INFO scheduler.TaskSetManager: Starting task 0.2 in stage 1.0 (TID 23, s12191, partition 0, NODE_LOCAL, 5241 bytes)
17/04/10 11:20:39 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 19 on executor id: 2 hostname: s12191.
17/04/10 11:20:39 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 20 on executor id: 2 hostname: s12191.
17/04/10 11:20:39 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 21 on executor id: 2 hostname: s12191.
17/04/10 11:20:39 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 22 on executor id: 2 hostname: s12191.
17/04/10 11:20:39 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 23 on executor id: 2 hostname: s12191.
17/04/10 11:20:46 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.4.121.92:40917) with ID 3
17/04/10 11:20:46 INFO storage.BlockManagerMasterEndpoint: Registering block manager s12192:48298 with 2004.6 MB RAM, BlockManagerId(3, s12192, 48298)
17/04/10 11:21:06 WARN scheduler.TaskSetManager: Lost task 3.1 in stage 1.0 (TID 16, s12191): FetchFailed(BlockManagerId(1, s12192, 52902), shuffleId=0, mapId=5, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to s12192/10.4.121.92:52902
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:357)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:332)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:54)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:154)
	at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:50)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:85)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to s12192/10.4.121.92:52902
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:96)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: s12192/10.4.121.92:52902
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
